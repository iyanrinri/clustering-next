{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 46, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/utils/stopwords.ts"],"sourcesContent":["// Indonesian and English stopwords for text preprocessing\n\nexport const indonesianStopwords = new Set([\n  'yang', 'dan', 'di', 'ke', 'dari', 'ini', 'itu', 'dengan', 'untuk', 'pada',\n  'adalah', 'ada', 'akan', 'atau', 'oleh', 'sebagai', 'dalam', 'juga', 'tidak',\n  'telah', 'dapat', 'sudah', 'saya', 'kami', 'kita', 'mereka', 'dia', 'ia',\n  'anda', 'kamu', 'nya', 'mu', 'ku', 'kah', 'lah', 'pun', 'per', 'si', 'sang',\n  'para', 'mana', 'bila', 'kalau', 'jika', 'apabila', 'agar', 'supaya', 'bisa',\n  'harus', 'perlu', 'mau', 'ingin', 'hendak', 'banyak', 'sedikit', 'semua',\n  'setiap', 'tiap', 'beberapa', 'suatu', 'satu', 'dua', 'tiga', 'empat', 'lima',\n  'enam', 'tujuh', 'delapan', 'sembilan', 'sepuluh', 'seratus', 'seribu',\n  'pertama', 'kedua', 'ketiga', 'sangat', 'lebih', 'paling', 'agak', 'cukup',\n  'terlalu', 'amat', 'begitu', 'demikian', 'sekali', 'nanti', 'kemudian',\n  'sekarang', 'sedang', 'masih', 'belum', 'pernah', 'sering', 'selalu', 'jarang',\n  // News/Journalism additions\n  'kepada', 'bila', 'seorang', 'tersebut', 'kata', 'saat', 'lalu', 'namun', \n  'tetapi', 'sehingga', 'karena', 'yaitu', 'yakni', 'adapun', 'bagaimana', 'apa',\n  'siapa', 'dimana', 'kapan', 'mengapa', 'hal', 'ini', 'itu', 'di', 'pada', 'juga'\n]);\n\nexport const englishStopwords = new Set([\n  'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 'for',\n  'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but', 'his',\n  'by', 'from', 'they', 'we', 'say', 'her', 'she', 'or', 'an', 'will', 'my',\n  'one', 'all', 'would', 'there', 'their', 'what', 'so', 'up', 'out', 'if',\n  'about', 'who', 'get', 'which', 'go', 'me', 'when', 'make', 'can', 'like',\n  'time', 'no', 'just', 'him', 'know', 'take', 'people', 'into', 'year', 'your',\n  'good', 'some', 'could', 'them', 'see', 'other', 'than', 'then', 'now', 'look',\n  'only', 'come', 'its', 'over', 'think', 'also', 'back', 'after', 'use', 'two',\n  'how', 'our', 'work', 'first', 'well', 'way', 'even', 'new', 'want', 'because',\n  'any', 'these', 'give', 'day', 'most', 'us', 'is', 'was', 'are', 'been', 'has',\n  'had', 'were', 'said', 'did', 'having', 'may', 'should', 'am', 'being', 'does'\n]);\n\nexport const combinedStopwords = new Set([\n  ...indonesianStopwords,\n  ...englishStopwords\n]);\n\nexport function isStopword(word: string, language: 'id' | 'en' | 'both' = 'both'): boolean {\n  const lowerWord = word.toLowerCase();\n  \n  switch (language) {\n    case 'id':\n      return indonesianStopwords.has(lowerWord);\n    case 'en':\n      return englishStopwords.has(lowerWord);\n    case 'both':\n    default:\n      return combinedStopwords.has(lowerWord);\n  }\n}\n"],"names":[],"mappings":"AAAA,0DAA0D;;;;;;;;;;;AAEnD,MAAM,sBAAsB,IAAI,IAAI;IACzC;IAAQ;IAAO;IAAM;IAAM;IAAQ;IAAO;IAAO;IAAU;IAAS;IACpE;IAAU;IAAO;IAAQ;IAAQ;IAAQ;IAAW;IAAS;IAAQ;IACrE;IAAS;IAAS;IAAS;IAAQ;IAAQ;IAAQ;IAAU;IAAO;IACpE;IAAQ;IAAQ;IAAO;IAAM;IAAM;IAAO;IAAO;IAAO;IAAO;IAAM;IACrE;IAAQ;IAAQ;IAAQ;IAAS;IAAQ;IAAW;IAAQ;IAAU;IACtE;IAAS;IAAS;IAAO;IAAS;IAAU;IAAU;IAAW;IACjE;IAAU;IAAQ;IAAY;IAAS;IAAQ;IAAO;IAAQ;IAAS;IACvE;IAAQ;IAAS;IAAW;IAAY;IAAW;IAAW;IAC9D;IAAW;IAAS;IAAU;IAAU;IAAS;IAAU;IAAQ;IACnE;IAAW;IAAQ;IAAU;IAAY;IAAU;IAAS;IAC5D;IAAY;IAAU;IAAS;IAAS;IAAU;IAAU;IAAU;IACtE,4BAA4B;IAC5B;IAAU;IAAQ;IAAW;IAAY;IAAQ;IAAQ;IAAQ;IACjE;IAAU;IAAY;IAAU;IAAS;IAAS;IAAU;IAAa;IACzE;IAAS;IAAU;IAAS;IAAW;IAAO;IAAO;IAAO;IAAM;IAAQ;CAC3E;AAEM,MAAM,mBAAmB,IAAI,IAAI;IACtC;IAAO;IAAM;IAAM;IAAM;IAAO;IAAK;IAAM;IAAQ;IAAQ;IAAK;IAAM;IACtE;IAAO;IAAM;IAAQ;IAAM;IAAM;IAAO;IAAM;IAAM;IAAQ;IAAO;IACnE;IAAM;IAAQ;IAAQ;IAAM;IAAO;IAAO;IAAO;IAAM;IAAM;IAAQ;IACrE;IAAO;IAAO;IAAS;IAAS;IAAS;IAAQ;IAAM;IAAM;IAAO;IACpE;IAAS;IAAO;IAAO;IAAS;IAAM;IAAM;IAAQ;IAAQ;IAAO;IACnE;IAAQ;IAAM;IAAQ;IAAO;IAAQ;IAAQ;IAAU;IAAQ;IAAQ;IACvE;IAAQ;IAAQ;IAAS;IAAQ;IAAO;IAAS;IAAQ;IAAQ;IAAO;IACxE;IAAQ;IAAQ;IAAO;IAAQ;IAAS;IAAQ;IAAQ;IAAS;IAAO;IACxE;IAAO;IAAO;IAAQ;IAAS;IAAQ;IAAO;IAAQ;IAAO;IAAQ;IACrE;IAAO;IAAS;IAAQ;IAAO;IAAQ;IAAM;IAAM;IAAO;IAAO;IAAQ;IACzE;IAAO;IAAQ;IAAQ;IAAO;IAAU;IAAO;IAAU;IAAM;IAAS;CACzE;AAEM,MAAM,oBAAoB,IAAI,IAAI;OACpC;OACA;CACJ;AAEM,SAAS,WAAW,IAAY,EAAE,WAAiC,MAAM;IAC9E,MAAM,YAAY,KAAK,WAAW;IAElC,OAAQ;QACN,KAAK;YACH,OAAO,oBAAoB,GAAG,CAAC;QACjC,KAAK;YACH,OAAO,iBAAiB,GAAG,CAAC;QAC9B,KAAK;QACL;YACE,OAAO,kBAAkB,GAAG,CAAC;IACjC;AACF"}},
    {"offset": {"line": 318, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/lib/clustering/preprocessor.ts"],"sourcesContent":["// Text preprocessing utilities\nimport { TokenType } from '../types';\nimport { combinedStopwords } from '@/utils/stopwords';\n\nexport function normalizeText(text: string): string {\n  return text\n    .toLowerCase()\n    .replace(/[^\\w\\s\\u00C0-\\u024F]/g, ' ') // Keep letters, numbers, spaces, and accented chars\n    .replace(/\\s+/g, ' ')\n    .trim();\n}\n\nexport function tokenizeByWord(text: string): string[] {\n  const normalized = normalizeText(text);\n  return normalized.split(/\\s+/).filter(word => word.length > 0);\n}\n\nexport function tokenizeByPhrase(text: string, phraseLength: number = 2): string[] {\n  const words = tokenizeByWord(text);\n  const phrases: string[] = [];\n  \n  for (let i = 0; i <= words.length - phraseLength; i++) {\n    const phrase = words.slice(i, i + phraseLength).join(' ');\n    phrases.push(phrase);\n  }\n  \n  return phrases;\n}\n\nexport function tokenizeBySentence(text: string): string[] {\n  // Split by sentence delimiters\n  const sentences = text\n    .split(/[.!?]+/)\n    .map(s => s.trim())\n    .filter(s => s.length > 0);\n  \n  return sentences.map(normalizeText);\n}\n\nexport function tokenize(text: string, type: TokenType): string[] {\n  switch (type) {\n    case 'word':\n      return tokenizeByWord(text);\n    case 'phrase':\n      return tokenizeByPhrase(text);\n    case 'sentence':\n      return tokenizeBySentence(text);\n    default:\n      return tokenizeByWord(text);\n  }\n}\n\nexport function removeStopwords(tokens: string[]): string[] {\n  return tokens.filter(token => {\n    // For phrases and sentences, check if majority of words are not stopwords\n    const words = token.split(/\\s+/);\n    if (words.length === 1) {\n      return !combinedStopwords.has(token.toLowerCase()) && token.length > 2;\n    }\n    \n    const nonStopwordCount = words.filter(\n      word => !combinedStopwords.has(word.toLowerCase())\n    ).length;\n    \n    return nonStopwordCount / words.length > 0.5;\n  });\n}\n\nexport function preprocessText(text: string, tokenType: TokenType): string[] {\n  const tokens = tokenize(text, tokenType);\n  const filtered = removeStopwords(tokens);\n  \n  // Remove duplicates while preserving order\n  return Array.from(new Set(filtered));\n}\n"],"names":[],"mappings":"AAAA,+BAA+B;;;;;;;;;;;;;;;;;AAE/B;;AAEO,SAAS,cAAc,IAAY;IACxC,OAAO,KACJ,WAAW,GACX,OAAO,CAAC,yBAAyB,KAAK,oDAAoD;KAC1F,OAAO,CAAC,QAAQ,KAChB,IAAI;AACT;AAEO,SAAS,eAAe,IAAY;IACzC,MAAM,aAAa,cAAc;IACjC,OAAO,WAAW,KAAK,CAAC,OAAO,MAAM,CAAC,CAAA,OAAQ,KAAK,MAAM,GAAG;AAC9D;AAEO,SAAS,iBAAiB,IAAY,EAAE,eAAuB,CAAC;IACrE,MAAM,QAAQ,eAAe;IAC7B,MAAM,UAAoB,EAAE;IAE5B,IAAK,IAAI,IAAI,GAAG,KAAK,MAAM,MAAM,GAAG,cAAc,IAAK;QACrD,MAAM,SAAS,MAAM,KAAK,CAAC,GAAG,IAAI,cAAc,IAAI,CAAC;QACrD,QAAQ,IAAI,CAAC;IACf;IAEA,OAAO;AACT;AAEO,SAAS,mBAAmB,IAAY;IAC7C,+BAA+B;IAC/B,MAAM,YAAY,KACf,KAAK,CAAC,UACN,GAAG,CAAC,CAAA,IAAK,EAAE,IAAI,IACf,MAAM,CAAC,CAAA,IAAK,EAAE,MAAM,GAAG;IAE1B,OAAO,UAAU,GAAG,CAAC;AACvB;AAEO,SAAS,SAAS,IAAY,EAAE,IAAe;IACpD,OAAQ;QACN,KAAK;YACH,OAAO,eAAe;QACxB,KAAK;YACH,OAAO,iBAAiB;QAC1B,KAAK;YACH,OAAO,mBAAmB;QAC5B;YACE,OAAO,eAAe;IAC1B;AACF;AAEO,SAAS,gBAAgB,MAAgB;IAC9C,OAAO,OAAO,MAAM,CAAC,CAAA;QACnB,0EAA0E;QAC1E,MAAM,QAAQ,MAAM,KAAK,CAAC;QAC1B,IAAI,MAAM,MAAM,KAAK,GAAG;YACtB,OAAO,CAAC,yIAAiB,CAAC,GAAG,CAAC,MAAM,WAAW,OAAO,MAAM,MAAM,GAAG;QACvE;QAEA,MAAM,mBAAmB,MAAM,MAAM,CACnC,CAAA,OAAQ,CAAC,yIAAiB,CAAC,GAAG,CAAC,KAAK,WAAW,KAC/C,MAAM;QAER,OAAO,mBAAmB,MAAM,MAAM,GAAG;IAC3C;AACF;AAEO,SAAS,eAAe,IAAY,EAAE,SAAoB;IAC/D,MAAM,SAAS,SAAS,MAAM;IAC9B,MAAM,WAAW,gBAAgB;IAEjC,2CAA2C;IAC3C,OAAO,MAAM,IAAI,CAAC,IAAI,IAAI;AAC5B"}},
    {"offset": {"line": 392, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/lib/clustering/vectorizer.ts"],"sourcesContent":["// TF-IDF Vectorization implementation\nimport { VectorData } from '../types';\n\ninterface TermFrequency {\n  [term: string]: number;\n}\n\ninterface DocumentFrequency {\n  [term: string]: number;\n}\n\nexport function calculateTF(document: string): TermFrequency {\n  const words = document.split(/\\s+/);\n  const tf: TermFrequency = {};\n  const totalWords = words.length;\n  \n  for (const word of words) {\n    tf[word] = (tf[word] || 0) + 1;\n  }\n  \n  // Normalize by document length\n  for (const word in tf) {\n    tf[word] = tf[word] / totalWords;\n  }\n  \n  return tf;\n}\n\nexport function calculateIDF(documents: string[]): DocumentFrequency {\n  const df: DocumentFrequency = {};\n  const numDocuments = documents.length;\n  \n  for (const doc of documents) {\n    const uniqueWords = new Set(doc.split(/\\s+/));\n    for (const word of uniqueWords) {\n      df[word] = (df[word] || 0) + 1;\n    }\n  }\n  \n  const idf: DocumentFrequency = {};\n  for (const word in df) {\n    idf[word] = Math.log(numDocuments / df[word]);\n  }\n  \n  return idf;\n}\n\nexport function vectorize(tokens: string[]): VectorData {\n  const numDocs = tokens.length;\n  \n  if (numDocs === 0) {\n    return { tokens: [], vectors: [], vocabulary: [] };\n  }\n  \n  // Calculate IDF\n  const idf = calculateIDF(tokens);\n  const vocabulary = Object.keys(idf).sort();\n  const vocabIndex = new Map(vocabulary.map((word, idx) => [word, idx]));\n  \n  // Calculate TF-IDF vectors\n  const vectors: number[][] = [];\n  \n  for (const token of tokens) {\n    const tf = calculateTF(token);\n    const vector = new Array(vocabulary.length).fill(0);\n    \n    for (const word in tf) {\n      const idx = vocabIndex.get(word);\n      if (idx !== undefined) {\n        vector[idx] = tf[word] * idf[word];\n      }\n    }\n    \n    // L2 Normalize the vector to unit length\n    // This allows Euclidean distance to approximate Cosine similarity behavior\n    let magnitude = 0;\n    for (const val of vector) {\n        magnitude += val * val;\n    }\n    magnitude = Math.sqrt(magnitude);\n    \n    if (magnitude > 0) {\n        for (let i = 0; i < vector.length; i++) {\n            vector[i] = vector[i] / magnitude;\n        }\n    }\n    \n    vectors.push(vector);\n  }\n  \n  return { tokens, vectors, vocabulary };\n}\n\nexport function cosineSimilarity(vec1: number[], vec2: number[]): number {\n  let dotProduct = 0;\n  let norm1 = 0;\n  let norm2 = 0;\n  \n  for (let i = 0; i < vec1.length; i++) {\n    dotProduct += vec1[i] * vec2[i];\n    norm1 += vec1[i] * vec1[i];\n    norm2 += vec2[i] * vec2[i];\n  }\n  \n  if (norm1 === 0 || norm2 === 0) {\n    return 0;\n  }\n  \n  return dotProduct / (Math.sqrt(norm1) * Math.sqrt(norm2));\n}\n\nexport function euclideanDistance(vec1: number[], vec2: number[]): number {\n  let sum = 0;\n  for (let i = 0; i < vec1.length; i++) {\n    const diff = vec1[i] - vec2[i];\n    sum += diff * diff;\n  }\n  return Math.sqrt(sum);\n}\n"],"names":[],"mappings":"AAAA,sCAAsC;;;;;;;;;;;;;AAW/B,SAAS,YAAY,QAAgB;IAC1C,MAAM,QAAQ,SAAS,KAAK,CAAC;IAC7B,MAAM,KAAoB,CAAC;IAC3B,MAAM,aAAa,MAAM,MAAM;IAE/B,KAAK,MAAM,QAAQ,MAAO;QACxB,EAAE,CAAC,KAAK,GAAG,CAAC,EAAE,CAAC,KAAK,IAAI,CAAC,IAAI;IAC/B;IAEA,+BAA+B;IAC/B,IAAK,MAAM,QAAQ,GAAI;QACrB,EAAE,CAAC,KAAK,GAAG,EAAE,CAAC,KAAK,GAAG;IACxB;IAEA,OAAO;AACT;AAEO,SAAS,aAAa,SAAmB;IAC9C,MAAM,KAAwB,CAAC;IAC/B,MAAM,eAAe,UAAU,MAAM;IAErC,KAAK,MAAM,OAAO,UAAW;QAC3B,MAAM,cAAc,IAAI,IAAI,IAAI,KAAK,CAAC;QACtC,KAAK,MAAM,QAAQ,YAAa;YAC9B,EAAE,CAAC,KAAK,GAAG,CAAC,EAAE,CAAC,KAAK,IAAI,CAAC,IAAI;QAC/B;IACF;IAEA,MAAM,MAAyB,CAAC;IAChC,IAAK,MAAM,QAAQ,GAAI;QACrB,GAAG,CAAC,KAAK,GAAG,KAAK,GAAG,CAAC,eAAe,EAAE,CAAC,KAAK;IAC9C;IAEA,OAAO;AACT;AAEO,SAAS,UAAU,MAAgB;IACxC,MAAM,UAAU,OAAO,MAAM;IAE7B,IAAI,YAAY,GAAG;QACjB,OAAO;YAAE,QAAQ,EAAE;YAAE,SAAS,EAAE;YAAE,YAAY,EAAE;QAAC;IACnD;IAEA,gBAAgB;IAChB,MAAM,MAAM,aAAa;IACzB,MAAM,aAAa,OAAO,IAAI,CAAC,KAAK,IAAI;IACxC,MAAM,aAAa,IAAI,IAAI,WAAW,GAAG,CAAC,CAAC,MAAM,MAAQ;YAAC;YAAM;SAAI;IAEpE,2BAA2B;IAC3B,MAAM,UAAsB,EAAE;IAE9B,KAAK,MAAM,SAAS,OAAQ;QAC1B,MAAM,KAAK,YAAY;QACvB,MAAM,SAAS,IAAI,MAAM,WAAW,MAAM,EAAE,IAAI,CAAC;QAEjD,IAAK,MAAM,QAAQ,GAAI;YACrB,MAAM,MAAM,WAAW,GAAG,CAAC;YAC3B,IAAI,QAAQ,WAAW;gBACrB,MAAM,CAAC,IAAI,GAAG,EAAE,CAAC,KAAK,GAAG,GAAG,CAAC,KAAK;YACpC;QACF;QAEA,yCAAyC;QACzC,2EAA2E;QAC3E,IAAI,YAAY;QAChB,KAAK,MAAM,OAAO,OAAQ;YACtB,aAAa,MAAM;QACvB;QACA,YAAY,KAAK,IAAI,CAAC;QAEtB,IAAI,YAAY,GAAG;YACf,IAAK,IAAI,IAAI,GAAG,IAAI,OAAO,MAAM,EAAE,IAAK;gBACpC,MAAM,CAAC,EAAE,GAAG,MAAM,CAAC,EAAE,GAAG;YAC5B;QACJ;QAEA,QAAQ,IAAI,CAAC;IACf;IAEA,OAAO;QAAE;QAAQ;QAAS;IAAW;AACvC;AAEO,SAAS,iBAAiB,IAAc,EAAE,IAAc;IAC7D,IAAI,aAAa;IACjB,IAAI,QAAQ;IACZ,IAAI,QAAQ;IAEZ,IAAK,IAAI,IAAI,GAAG,IAAI,KAAK,MAAM,EAAE,IAAK;QACpC,cAAc,IAAI,CAAC,EAAE,GAAG,IAAI,CAAC,EAAE;QAC/B,SAAS,IAAI,CAAC,EAAE,GAAG,IAAI,CAAC,EAAE;QAC1B,SAAS,IAAI,CAAC,EAAE,GAAG,IAAI,CAAC,EAAE;IAC5B;IAEA,IAAI,UAAU,KAAK,UAAU,GAAG;QAC9B,OAAO;IACT;IAEA,OAAO,aAAa,CAAC,KAAK,IAAI,CAAC,SAAS,KAAK,IAAI,CAAC,MAAM;AAC1D;AAEO,SAAS,kBAAkB,IAAc,EAAE,IAAc;IAC9D,IAAI,MAAM;IACV,IAAK,IAAI,IAAI,GAAG,IAAI,KAAK,MAAM,EAAE,IAAK;QACpC,MAAM,OAAO,IAAI,CAAC,EAAE,GAAG,IAAI,CAAC,EAAE;QAC9B,OAAO,OAAO;IAChB;IACA,OAAO,KAAK,IAAI,CAAC;AACnB"}},
    {"offset": {"line": 506, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/lib/clustering/kmeans.ts"],"sourcesContent":["// K-means clustering implementation\nimport { kmeans } from 'ml-kmeans';\n\n// Custom euclidean distance function\nfunction euclideanDistance(a: number[], b: number[]): number {\n  let sum = 0;\n  for (let i = 0; i < a.length; i++) {\n    const diff = a[i] - b[i];\n    sum += diff * diff;\n  }\n  return Math.sqrt(sum);\n}\n\nexport interface KMeansResult {\n  clusters: number[];\n  centroids: number[][];\n  iterations: number;\n}\n\n// Simple seeded random number generator (Linear Congruential Generator)\nclass SeededRandom {\n  private seed: number;\n\n  constructor(seed: number) {\n    this.seed = seed;\n  }\n\n  next(): number {\n    // LCG constants\n    this.seed = (this.seed * 9301 + 49297) % 233280;\n    return this.seed / 233280;\n  }\n}\n\nexport function performKMeans(\n  vectors: number[][],\n  k: number,\n  options?: {\n    maxIterations?: number;\n    tolerance?: number;\n    seed?: number;\n    attempts?: number; // Number of times to run k-means with different seeds\n  }\n): KMeansResult {\n  const maxIterations = options?.maxIterations || 100;\n  const tolerance = options?.tolerance || 1e-4;\n  const baseSeed = options?.seed || 42;\n  const attempts = options?.attempts || 10; // Try 10 times by default\n  \n  if (vectors.length === 0) {\n    return { clusters: [], centroids: [], iterations: 0 };\n  }\n  \n  // Ensure k is not greater than number of vectors\n  const numClusters = Math.min(k, vectors.length);\n  \n  // Store the original Math.random\n  const originalRandom = Math.random;\n  \n  let bestResult: KMeansResult | null = null;\n  let minInertia = Infinity; // Lower is better (Within-Cluster Sum of Squares)\n\n  try {\n    for (let attempt = 0; attempt < attempts; attempt++) {\n      // Use different seed for each attempt but deterministic overall\n      const currentSeed = baseSeed + attempt * 7919; \n      const rng = new SeededRandom(currentSeed);\n      Math.random = () => rng.next();\n\n      try {\n        const result = kmeans(vectors, numClusters, {\n          maxIterations,\n          tolerance,\n          distanceFunction: euclideanDistance,\n        });\n\n        // Calculate Inertia (Sum of squared distances to closest centroid)\n        // WCSS = Sum(Distance(points, assigned_centroid)^2)\n        let currentInertia = 0;\n        for (let i = 0; i < vectors.length; i++) {\n           const centroid = result.centroids[result.clusters[i]];\n           const dist = euclideanDistance(vectors[i], centroid);\n           currentInertia += dist * dist;\n        }\n\n        // If this attempt is better, or first attempts, keep it\n        // We also punish \"empty\" clusters or super lop-sided ones if we wanted to forced balance\n        // but inertia naturally favors compact clusters.\n        \n        // Simple heuristic: If we have ANY empty clusters (rare with k-means but possible), \n        // treat inertia as high? No, ml-kmeans usually avoids empty.\n        \n        if (currentInertia < minInertia) {\n          minInertia = currentInertia;\n          bestResult = {\n            clusters: result.clusters,\n            centroids: result.centroids,\n            iterations: result.iterations,\n          };\n        }\n      } catch (err) {\n        console.warn(`K-Means attempt ${attempt} failed`, err);\n        // Continue to next attempt\n      }\n    }\n    \n    if (!bestResult) {\n      throw new Error('All K-Means attempts failed');\n    }\n    \n    return bestResult;\n\n  } catch (error) {\n    console.error('K-means clustering error:', error);\n    // Fallback: assign randomly or all to 0\n    return {\n      clusters: new Array(vectors.length).fill(0),\n      centroids: [vectors[0] || []],\n      iterations: 0,\n    };\n  } finally {\n    // Always restore original Math.random\n    Math.random = originalRandom;\n  }\n}\n\nexport function findClosestCentroid(vector: number[], centroids: number[][]): number {\n  let minDistance = Infinity;\n  let closestIndex = 0;\n  \n  for (let i = 0; i < centroids.length; i++) {\n    const dist = euclideanDistance(vector, centroids[i]);\n    if (dist < minDistance) {\n      minDistance = dist;\n      closestIndex = i;\n    }\n  }\n  \n  return closestIndex;\n}\n"],"names":[],"mappings":";;;;;;AAAA,oCAAoC;AACpC;AAAA;;AAEA,qCAAqC;AACrC,SAAS,kBAAkB,CAAW,EAAE,CAAW;IACjD,IAAI,MAAM;IACV,IAAK,IAAI,IAAI,GAAG,IAAI,EAAE,MAAM,EAAE,IAAK;QACjC,MAAM,OAAO,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC,EAAE;QACxB,OAAO,OAAO;IAChB;IACA,OAAO,KAAK,IAAI,CAAC;AACnB;AAQA,wEAAwE;AACxE,MAAM;IACI,KAAa;IAErB,YAAY,IAAY,CAAE;QACxB,IAAI,CAAC,IAAI,GAAG;IACd;IAEA,OAAe;QACb,gBAAgB;QAChB,IAAI,CAAC,IAAI,GAAG,CAAC,IAAI,CAAC,IAAI,GAAG,OAAO,KAAK,IAAI;QACzC,OAAO,IAAI,CAAC,IAAI,GAAG;IACrB;AACF;AAEO,SAAS,cACd,OAAmB,EACnB,CAAS,EACT,OAKC;IAED,MAAM,gBAAgB,SAAS,iBAAiB;IAChD,MAAM,YAAY,SAAS,aAAa;IACxC,MAAM,WAAW,SAAS,QAAQ;IAClC,MAAM,WAAW,SAAS,YAAY,IAAI,0BAA0B;IAEpE,IAAI,QAAQ,MAAM,KAAK,GAAG;QACxB,OAAO;YAAE,UAAU,EAAE;YAAE,WAAW,EAAE;YAAE,YAAY;QAAE;IACtD;IAEA,iDAAiD;IACjD,MAAM,cAAc,KAAK,GAAG,CAAC,GAAG,QAAQ,MAAM;IAE9C,iCAAiC;IACjC,MAAM,iBAAiB,KAAK,MAAM;IAElC,IAAI,aAAkC;IACtC,IAAI,aAAa,UAAU,kDAAkD;IAE7E,IAAI;QACF,IAAK,IAAI,UAAU,GAAG,UAAU,UAAU,UAAW;YACnD,gEAAgE;YAChE,MAAM,cAAc,WAAW,UAAU;YACzC,MAAM,MAAM,IAAI,aAAa;YAC7B,KAAK,MAAM,GAAG,IAAM,IAAI,IAAI;YAE5B,IAAI;gBACF,MAAM,SAAS,IAAA,yJAAM,EAAC,SAAS,aAAa;oBAC1C;oBACA;oBACA,kBAAkB;gBACpB;gBAEA,mEAAmE;gBACnE,oDAAoD;gBACpD,IAAI,iBAAiB;gBACrB,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,MAAM,EAAE,IAAK;oBACtC,MAAM,WAAW,OAAO,SAAS,CAAC,OAAO,QAAQ,CAAC,EAAE,CAAC;oBACrD,MAAM,OAAO,kBAAkB,OAAO,CAAC,EAAE,EAAE;oBAC3C,kBAAkB,OAAO;gBAC5B;gBAEA,wDAAwD;gBACxD,yFAAyF;gBACzF,iDAAiD;gBAEjD,qFAAqF;gBACrF,6DAA6D;gBAE7D,IAAI,iBAAiB,YAAY;oBAC/B,aAAa;oBACb,aAAa;wBACX,UAAU,OAAO,QAAQ;wBACzB,WAAW,OAAO,SAAS;wBAC3B,YAAY,OAAO,UAAU;oBAC/B;gBACF;YACF,EAAE,OAAO,KAAK;gBACZ,QAAQ,IAAI,CAAC,CAAC,gBAAgB,EAAE,QAAQ,OAAO,CAAC,EAAE;YAClD,2BAA2B;YAC7B;QACF;QAEA,IAAI,CAAC,YAAY;YACf,MAAM,IAAI,MAAM;QAClB;QAEA,OAAO;IAET,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,6BAA6B;QAC3C,wCAAwC;QACxC,OAAO;YACL,UAAU,IAAI,MAAM,QAAQ,MAAM,EAAE,IAAI,CAAC;YACzC,WAAW;gBAAC,OAAO,CAAC,EAAE,IAAI,EAAE;aAAC;YAC7B,YAAY;QACd;IACF,SAAU;QACR,sCAAsC;QACtC,KAAK,MAAM,GAAG;IAChB;AACF;AAEO,SAAS,oBAAoB,MAAgB,EAAE,SAAqB;IACzE,IAAI,cAAc;IAClB,IAAI,eAAe;IAEnB,IAAK,IAAI,IAAI,GAAG,IAAI,UAAU,MAAM,EAAE,IAAK;QACzC,MAAM,OAAO,kBAAkB,QAAQ,SAAS,CAAC,EAAE;QACnD,IAAI,OAAO,aAAa;YACtB,cAAc;YACd,eAAe;QACjB;IACF;IAEA,OAAO;AACT"}},
    {"offset": {"line": 628, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/lib/clustering/hierarchical.ts"],"sourcesContent":["// Hierarchical clustering implementation\nimport { euclideanDistance } from './vectorizer';\n\ninterface ClusterNode {\n  left?: ClusterNode;\n  right?: ClusterNode;\n  distance: number;\n  indices: number[];\n}\n\nexport function performHierarchical(\n  vectors: number[][],\n  k: number\n): number[] {\n  if (vectors.length === 0) {\n    return [];\n  }\n  \n  if (vectors.length <= k) {\n    return vectors.map((_, idx) => idx);\n  }\n  \n  // Build dendrogram using agglomerative clustering\n  const dendrogram = buildDendrogram(vectors);\n  \n  // Cut dendrogram to get k clusters\n  return cutDendrogram(dendrogram, k, vectors.length);\n}\n\nfunction buildDendrogram(vectors: number[][]): ClusterNode {\n  // Initialize each point as a cluster\n  let clusters: ClusterNode[] = vectors.map((_, idx) => ({\n    distance: 0,\n    indices: [idx],\n  }));\n  \n  // Merge clusters until only one remains\n  while (clusters.length > 1) {\n    // Find closest pair of clusters\n    let minDistance = Infinity;\n    let mergeI = 0;\n    let mergeJ = 1;\n    \n    for (let i = 0; i < clusters.length; i++) {\n      for (let j = i + 1; j < clusters.length; j++) {\n        const distance = clusterDistance(clusters[i], clusters[j], vectors);\n        if (distance < minDistance) {\n          minDistance = distance;\n          mergeI = i;\n          mergeJ = j;\n        }\n      }\n    }\n    \n    // Merge the closest clusters\n    const newCluster: ClusterNode = {\n      left: clusters[mergeI],\n      right: clusters[mergeJ],\n      distance: minDistance,\n      indices: [...clusters[mergeI].indices, ...clusters[mergeJ].indices],\n    };\n    \n    // Remove old clusters and add new one\n    clusters = clusters.filter((_, idx) => idx !== mergeI && idx !== mergeJ);\n    clusters.push(newCluster);\n  }\n  \n  return clusters[0];\n}\n\nfunction clusterDistance(\n  cluster1: ClusterNode,\n  cluster2: ClusterNode,\n  vectors: number[][]\n): number {\n  // Average linkage: average distance between all pairs\n  let totalDistance = 0;\n  let count = 0;\n  \n  for (const i of cluster1.indices) {\n    for (const j of cluster2.indices) {\n      totalDistance += euclideanDistance(vectors[i], vectors[j]);\n      count++;\n    }\n  }\n  \n  return count > 0 ? totalDistance / count : 0;\n}\n\nfunction cutDendrogram(\n  root: ClusterNode,\n  k: number,\n  totalPoints: number\n): number[] {\n  // Assign cluster IDs to each point\n  const assignments = new Array(totalPoints).fill(-1);\n  \n  // Get k clusters by cutting the dendrogram\n  const clusters = getClusters(root, k);\n  \n  // Assign cluster IDs\n  clusters.forEach((cluster, clusterId) => {\n    for (const idx of cluster.indices) {\n      assignments[idx] = clusterId;\n    }\n  });\n  \n  return assignments;\n}\n\nfunction getClusters(root: ClusterNode, k: number): ClusterNode[] {\n  if (k === 1) {\n    return [root];\n  }\n  \n  const queue: ClusterNode[] = [root];\n  const result: ClusterNode[] = [];\n  \n  while (queue.length + result.length < k && queue.length > 0) {\n    // Sort by distance (descending) to split largest clusters first\n    queue.sort((a, b) => b.distance - a.distance);\n    \n    const cluster = queue.shift()!;\n    \n    if (cluster.left && cluster.right) {\n      queue.push(cluster.left, cluster.right);\n    } else {\n      result.push(cluster);\n    }\n  }\n  \n  // Add remaining clusters\n  result.push(...queue);\n  \n  return result;\n}\n"],"names":[],"mappings":";;;;AAAA,yCAAyC;AACzC;;AASO,SAAS,oBACd,OAAmB,EACnB,CAAS;IAET,IAAI,QAAQ,MAAM,KAAK,GAAG;QACxB,OAAO,EAAE;IACX;IAEA,IAAI,QAAQ,MAAM,IAAI,GAAG;QACvB,OAAO,QAAQ,GAAG,CAAC,CAAC,GAAG,MAAQ;IACjC;IAEA,kDAAkD;IAClD,MAAM,aAAa,gBAAgB;IAEnC,mCAAmC;IACnC,OAAO,cAAc,YAAY,GAAG,QAAQ,MAAM;AACpD;AAEA,SAAS,gBAAgB,OAAmB;IAC1C,qCAAqC;IACrC,IAAI,WAA0B,QAAQ,GAAG,CAAC,CAAC,GAAG,MAAQ,CAAC;YACrD,UAAU;YACV,SAAS;gBAAC;aAAI;QAChB,CAAC;IAED,wCAAwC;IACxC,MAAO,SAAS,MAAM,GAAG,EAAG;QAC1B,gCAAgC;QAChC,IAAI,cAAc;QAClB,IAAI,SAAS;QACb,IAAI,SAAS;QAEb,IAAK,IAAI,IAAI,GAAG,IAAI,SAAS,MAAM,EAAE,IAAK;YACxC,IAAK,IAAI,IAAI,IAAI,GAAG,IAAI,SAAS,MAAM,EAAE,IAAK;gBAC5C,MAAM,WAAW,gBAAgB,QAAQ,CAAC,EAAE,EAAE,QAAQ,CAAC,EAAE,EAAE;gBAC3D,IAAI,WAAW,aAAa;oBAC1B,cAAc;oBACd,SAAS;oBACT,SAAS;gBACX;YACF;QACF;QAEA,6BAA6B;QAC7B,MAAM,aAA0B;YAC9B,MAAM,QAAQ,CAAC,OAAO;YACtB,OAAO,QAAQ,CAAC,OAAO;YACvB,UAAU;YACV,SAAS;mBAAI,QAAQ,CAAC,OAAO,CAAC,OAAO;mBAAK,QAAQ,CAAC,OAAO,CAAC,OAAO;aAAC;QACrE;QAEA,sCAAsC;QACtC,WAAW,SAAS,MAAM,CAAC,CAAC,GAAG,MAAQ,QAAQ,UAAU,QAAQ;QACjE,SAAS,IAAI,CAAC;IAChB;IAEA,OAAO,QAAQ,CAAC,EAAE;AACpB;AAEA,SAAS,gBACP,QAAqB,EACrB,QAAqB,EACrB,OAAmB;IAEnB,sDAAsD;IACtD,IAAI,gBAAgB;IACpB,IAAI,QAAQ;IAEZ,KAAK,MAAM,KAAK,SAAS,OAAO,CAAE;QAChC,KAAK,MAAM,KAAK,SAAS,OAAO,CAAE;YAChC,iBAAiB,IAAA,sJAAiB,EAAC,OAAO,CAAC,EAAE,EAAE,OAAO,CAAC,EAAE;YACzD;QACF;IACF;IAEA,OAAO,QAAQ,IAAI,gBAAgB,QAAQ;AAC7C;AAEA,SAAS,cACP,IAAiB,EACjB,CAAS,EACT,WAAmB;IAEnB,mCAAmC;IACnC,MAAM,cAAc,IAAI,MAAM,aAAa,IAAI,CAAC,CAAC;IAEjD,2CAA2C;IAC3C,MAAM,WAAW,YAAY,MAAM;IAEnC,qBAAqB;IACrB,SAAS,OAAO,CAAC,CAAC,SAAS;QACzB,KAAK,MAAM,OAAO,QAAQ,OAAO,CAAE;YACjC,WAAW,CAAC,IAAI,GAAG;QACrB;IACF;IAEA,OAAO;AACT;AAEA,SAAS,YAAY,IAAiB,EAAE,CAAS;IAC/C,IAAI,MAAM,GAAG;QACX,OAAO;YAAC;SAAK;IACf;IAEA,MAAM,QAAuB;QAAC;KAAK;IACnC,MAAM,SAAwB,EAAE;IAEhC,MAAO,MAAM,MAAM,GAAG,OAAO,MAAM,GAAG,KAAK,MAAM,MAAM,GAAG,EAAG;QAC3D,gEAAgE;QAChE,MAAM,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,QAAQ,GAAG,EAAE,QAAQ;QAE5C,MAAM,UAAU,MAAM,KAAK;QAE3B,IAAI,QAAQ,IAAI,IAAI,QAAQ,KAAK,EAAE;YACjC,MAAM,IAAI,CAAC,QAAQ,IAAI,EAAE,QAAQ,KAAK;QACxC,OAAO;YACL,OAAO,IAAI,CAAC;QACd;IACF;IAEA,yBAAyB;IACzB,OAAO,IAAI,IAAI;IAEf,OAAO;AACT"}},
    {"offset": {"line": 740, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/utils/colors.ts"],"sourcesContent":["// Color palette generator for cluster visualization\nimport { schemeCategory10, schemeSet3, schemePaired } from 'd3-scale-chromatic';\n\nexport const clusterColors = [\n  '#3b82f6', // Blue\n  '#8b5cf6', // Purple\n  '#ec4899', // Pink\n  '#f59e0b', // Amber\n  '#10b981', // Emerald\n  '#06b6d4', // Cyan\n  '#f97316', // Orange\n  '#6366f1', // Indigo\n  '#14b8a6', // Teal\n  '#ef4444', // Red\n  '#84cc16', // Lime\n  '#a855f7', // Violet\n];\n\nexport function getClusterColor(clusterId: number): string {\n  return clusterColors[clusterId % clusterColors.length];\n}\n\nexport function generateColorPalette(numClusters: number): string[] {\n  if (numClusters <= clusterColors.length) {\n    return clusterColors.slice(0, numClusters);\n  }\n  \n  // For more clusters, use d3 color schemes\n  const colors: string[] = [];\n  const schemes = [schemeCategory10, schemeSet3, schemePaired];\n  \n  for (let i = 0; i < numClusters; i++) {\n    const schemeIndex = Math.floor(i / 10) % schemes.length;\n    const colorIndex = i % 10;\n    colors.push(schemes[schemeIndex][colorIndex]);\n  }\n  \n  return colors;\n}\n\nexport function getNodeColor(node: { cluster: number }, opacity: number = 1): string {\n  const baseColor = getClusterColor(node.cluster);\n  \n  if (opacity === 1) {\n    return baseColor;\n  }\n  \n  // Convert hex to rgba\n  const r = parseInt(baseColor.slice(1, 3), 16);\n  const g = parseInt(baseColor.slice(3, 5), 16);\n  const b = parseInt(baseColor.slice(5, 7), 16);\n  \n  return `rgba(${r}, ${g}, ${b}, ${opacity})`;\n}\n"],"names":[],"mappings":";;;;;;;;;;AAAA,oDAAoD;AACpD;AAAA;AAAA;;AAEO,MAAM,gBAAgB;IAC3B;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;CACD;AAEM,SAAS,gBAAgB,SAAiB;IAC/C,OAAO,aAAa,CAAC,YAAY,cAAc,MAAM,CAAC;AACxD;AAEO,SAAS,qBAAqB,WAAmB;IACtD,IAAI,eAAe,cAAc,MAAM,EAAE;QACvC,OAAO,cAAc,KAAK,CAAC,GAAG;IAChC;IAEA,0CAA0C;IAC1C,MAAM,SAAmB,EAAE;IAC3B,MAAM,UAAU;QAAC,iPAAgB;QAAE,+NAAU;QAAE,qOAAY;KAAC;IAE5D,IAAK,IAAI,IAAI,GAAG,IAAI,aAAa,IAAK;QACpC,MAAM,cAAc,KAAK,KAAK,CAAC,IAAI,MAAM,QAAQ,MAAM;QACvD,MAAM,aAAa,IAAI;QACvB,OAAO,IAAI,CAAC,OAAO,CAAC,YAAY,CAAC,WAAW;IAC9C;IAEA,OAAO;AACT;AAEO,SAAS,aAAa,IAAyB,EAAE,UAAkB,CAAC;IACzE,MAAM,YAAY,gBAAgB,KAAK,OAAO;IAE9C,IAAI,YAAY,GAAG;QACjB,OAAO;IACT;IAEA,sBAAsB;IACtB,MAAM,IAAI,SAAS,UAAU,KAAK,CAAC,GAAG,IAAI;IAC1C,MAAM,IAAI,SAAS,UAAU,KAAK,CAAC,GAAG,IAAI;IAC1C,MAAM,IAAI,SAAS,UAAU,KAAK,CAAC,GAAG,IAAI;IAE1C,OAAO,CAAC,KAAK,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,EAAE,QAAQ,CAAC,CAAC;AAC7C"}},
    {"offset": {"line": 805, "column": 0}, "map": {"version":3,"sources":["file:///Users/iyan/NodeStudio/clustering-next/app/api/cluster/route.ts"],"sourcesContent":["// API Route for text clustering\nimport { NextRequest, NextResponse } from 'next/server';\nimport { ClusterRequest, ClusterResult, ClusterNode, ClusterLink, ClusterInfo } from '@/lib/types';\nimport { preprocessText, tokenize, removeStopwords } from '@/lib/clustering/preprocessor';\nimport { vectorize, cosineSimilarity } from '@/lib/clustering/vectorizer';\nimport { performKMeans } from '@/lib/clustering/kmeans';\nimport { performHierarchical } from '@/lib/clustering/hierarchical';\nimport { getClusterColor } from '@/utils/colors';\n\nexport async function POST(request: NextRequest) {\n  const startTime = Date.now();\n  \n  try {\n    const body: ClusterRequest = await request.json();\n    const { text, algorithm, numClusters, tokenType } = body;\n    \n    // Validate input\n    if (!text || text.trim().length === 0) {\n      return NextResponse.json(\n        { error: 'Text input is required' },\n        { status: 400 }\n      );\n    }\n    \n    if (numClusters < 2 || numClusters > 20) {\n      return NextResponse.json(\n        { error: 'Number of clusters must be between 2 and 20' },\n        { status: 400 }\n      );\n    }\n    \n    // Step 1: Preprocess text\n    const tokens = preprocessText(text, tokenType);\n    \n    if (tokens.length === 0) {\n      return NextResponse.json(\n        { error: 'No valid tokens found after preprocessing' },\n        { status: 400 }\n      );\n    }\n    \n    if (tokens.length < numClusters) {\n      return NextResponse.json(\n        { \n          error: `Not enough tokens (${tokens.length}) for ${numClusters} clusters. Try reducing the number of clusters or using a different token type.` \n        },\n        { status: 400 }\n      );\n    }\n    \n    // Step 2: Vectorization Strategy\n    // CRITICAL FIX: If we are clustering WORDS, we cannot treat them as independent documents.\n    // We must cluster them based on their CO-OCCURRENCE in sentences.\n    // Otherwise, every word is orthogonal to every other word (Distance = 1.0), causing the \"Giant Cluster\" issue.\n    \n    let vectors: number[][];\n    let vocabulary: string[];\n    let clusterItems: string[];\n\n    if (tokenType === 'word') {\n       // 1. Split text into context windows (sentences)\n       const sentences = preprocessText(text, 'sentence');\n       \n       // 2. Get unique words to cluster\n       clusterItems = Array.from(new Set(tokens)); \n       \n       if (clusterItems.length < numClusters) {\n          return NextResponse.json(\n            { error: `Not enough unique words (${clusterItems.length}) for ${numClusters} clusters.` },\n            { status: 400 }\n          );\n       }\n\n       // 3. Build Co-occurrence Matrix\n       // Rows = Words, Cols = Sentences\n       // If word W appears in Sentence S, value is 1 (or tf-idf in S)\n       \n       vectors = clusterItems.map(word => {\n          const vec = new Array(sentences.length).fill(0);\n\n          sentences.forEach((sentence, sIdx) => {\n             // Simple regex to match whole word to avoid substr issues (e.g. 'cat' in 'catch')\n             // Escape special logic not really needed for simpler word matching here \n             // but 'includes' is safer for now. Ideally use tokenized sentence set.\n             if (sentence.toLowerCase().includes(word.toLowerCase())) {\n                vec[sIdx] = 1; // Simple binary occurrence\n             }\n          });\n          \n          // L2 Normalize row\n          let mag = 0;\n          for(const v of vec) mag += v*v;\n          mag = Math.sqrt(mag);\n          if (mag > 0) {\n             for(let i=0; i<vec.length; i++) vec[i] /= mag;\n          }\n\n          return vec;\n       });\n\n       // Vocabulary for \"Word Clustering\" is actually the Sentences (Technically)\n       vocabulary = sentences;\n\n    } else {\n       // Standard TF-IDF Document Clustering (for Sentences/Paragraphs)\n       clusterItems = tokens;\n       const vectResult = vectorize(tokens);\n       vectors = vectResult.vectors;\n       vocabulary = vectResult.vocabulary;\n    }\n    \n    // Step 3: Perform clustering\n    let clusterAssignments: number[];\n    \n    if (algorithm === 'kmeans') {\n      const result = performKMeans(vectors, numClusters, { attempts: 15 });\n      clusterAssignments = result.clusters;\n    } else {\n      clusterAssignments = performHierarchical(vectors, numClusters);\n    }\n    \n    // Step 4: Build graph structure\n    const nodes: ClusterNode[] = clusterItems.map((token, idx) => ({\n      id: `node_${idx}`,\n      text: token,\n      cluster: clusterAssignments[idx],\n    }));\n    \n    // Step 5: Create links based on similarity\n    // Reuse existing link logic but map indices correctly\n    const links: ClusterLink[] = [];\n    const similarityThreshold = 0.2; // Lower threshold for co-occurence\n    \n    for (let i = 0; i < vectors.length; i++) {\n      for (let j = i + 1; j < vectors.length; j++) {\n        const similarity = cosineSimilarity(vectors[i], vectors[j]);\n        \n        if (similarity > similarityThreshold) {\n          links.push({\n            source: `node_${i}`,\n            target: `node_${j}`,\n            value: similarity,\n          });\n        }\n      }\n    }\n    \n    // Step 6: Calculate cluster statistics\n    const clusterMap = new Map<number, number[]>();\n    clusterAssignments.forEach((cluster, idx) => {\n      if (!clusterMap.has(cluster)) {\n        clusterMap.set(cluster, []);\n      }\n      clusterMap.get(cluster)!.push(idx);\n    });\n    \n    const clusters: ClusterInfo[] = Array.from(clusterMap.entries()).map(([id, indices]) => {\n      // For Word clustering, the \"top words\" are the words themselves in the cluster\n      let clusterWords: { text: string; weight: number }[];\n      \n      if (tokenType === 'word') {\n          // Calculate frequency/weight for sorting\n          // Since we don't have global TF-IDF easily accessible here for the words (we used sentences as docs),\n          // let's use the Raw Frequency of the word in the original text or just the raw occurrences in sentences.\n          \n          // We can re-scan sentences for these specific words efficiently or just assume\n          // we want to highlight the most \"connected\" words.\n          // But Frequency is easiest and expected.\n          \n          // Actually, we can assume 'token' array has all word occurrences if we tokenized by word?\n          // Wait, 'tokens' input to this route is: const tokens = preprocessText(text, tokenType);\n          // If tokenType='word', 'tokens' contains EVERY word instance (e.g. ['jakarta', 'seorang', 'oknum', ...])\n          // So we can just count occurrences in 'tokens'!\n          \n          // Re-tokenize raw text to get actual word counts (including duplicates)\n          // preprocessText() returns unique set, so we can't use 'tokens' variable for counts.\n          const rawTokens = removeStopwords(tokenize(text, tokenType));\n          \n          const freqMap = new Map<string, number>();\n          rawTokens.forEach(t => {\n             freqMap.set(t, (freqMap.get(t) || 0) + 1);\n          });\n\n          clusterWords = indices.map(idx => ({\n              text: clusterItems[idx],\n              weight: freqMap.get(clusterItems[idx]) || 1\n          }))\n          .sort((a, b) => b.weight - a.weight) // Sort by frequency\n          .slice(0, 15);\n          \n      } else {\n          // Use original logic for Sentence clustering\n          clusterWords = calculateClusterWords(indices, clusterItems, vectors, vocabulary, 10);\n      }\n\n      return {\n        id,\n        size: indices.length,\n        words: clusterWords,\n        color: getClusterColor(id),\n      };\n    });\n    \n    const processingTime = Date.now() - startTime;\n    \n    const result: ClusterResult = {\n      nodes,\n      links,\n      clusters,\n      totalTokens: clusterItems.length,\n      processingTime,\n      algorithm: algorithm === 'kmeans' ? 'K-Means' : 'Hierarchical',\n    };\n    \n    return NextResponse.json(result);\n    \n  } catch (error) {\n    console.error('Clustering error:', error);\n    return NextResponse.json(\n      { error: 'Internal server error during clustering' },\n      { status: 500 }\n    );\n  }\n}\n\nfunction calculateClusterWords(\n  indices: number[],\n  tokens: string[],\n  vectors: number[][],\n  vocabulary: string[],\n  topN: number\n): { text: string; weight: number }[] {\n  // Calculate average TF-IDF scores for each word in the cluster\n  const wordScores = new Map<string, number>();\n  \n  for (const idx of indices) {\n    const token = tokens[idx];\n    const vector = vectors[idx];\n    \n    // Extract words from token and their TF-IDF scores\n    const words = token.split(/\\s+/);\n    words.forEach(word => {\n      const vocabIdx = vocabulary.indexOf(word);\n      if (vocabIdx !== -1) {\n        const tfidfScore = vector[vocabIdx];\n        const currentScore = wordScores.get(word) || 0;\n        wordScores.set(word, currentScore + tfidfScore);\n      }\n    });\n  }\n  \n  // Calculate average scores\n  const clusterSize = indices.length;\n  wordScores.forEach((score, word) => {\n    wordScores.set(word, score / clusterSize);\n  });\n  \n  // Sort by weight and return top N\n  return Array.from(wordScores.entries())\n    .sort((a, b) => b[1] - a[1])\n    .slice(0, topN)\n    .map(([text, weight]) => ({\n      text,\n      weight: Math.round(weight * 100) / 100, // Round to 2 decimal places\n    }));\n}\n"],"names":[],"mappings":";;;;AAAA,gCAAgC;AAChC;AAEA;AACA;AACA;AACA;AACA;;;;;;;AAEO,eAAe,KAAK,OAAoB;IAC7C,MAAM,YAAY,KAAK,GAAG;IAE1B,IAAI;QACF,MAAM,OAAuB,MAAM,QAAQ,IAAI;QAC/C,MAAM,EAAE,IAAI,EAAE,SAAS,EAAE,WAAW,EAAE,SAAS,EAAE,GAAG;QAEpD,iBAAiB;QACjB,IAAI,CAAC,QAAQ,KAAK,IAAI,GAAG,MAAM,KAAK,GAAG;YACrC,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAyB,GAClC;gBAAE,QAAQ;YAAI;QAElB;QAEA,IAAI,cAAc,KAAK,cAAc,IAAI;YACvC,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAA8C,GACvD;gBAAE,QAAQ;YAAI;QAElB;QAEA,0BAA0B;QAC1B,MAAM,SAAS,IAAA,qJAAc,EAAC,MAAM;QAEpC,IAAI,OAAO,MAAM,KAAK,GAAG;YACvB,OAAO,gJAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAA4C,GACrD;gBAAE,QAAQ;YAAI;QAElB;QAEA,IAAI,OAAO,MAAM,GAAG,aAAa;YAC/B,OAAO,gJAAY,CAAC,IAAI,CACtB;gBACE,OAAO,CAAC,mBAAmB,EAAE,OAAO,MAAM,CAAC,MAAM,EAAE,YAAY,+EAA+E,CAAC;YACjJ,GACA;gBAAE,QAAQ;YAAI;QAElB;QAEA,iCAAiC;QACjC,2FAA2F;QAC3F,kEAAkE;QAClE,+GAA+G;QAE/G,IAAI;QACJ,IAAI;QACJ,IAAI;QAEJ,IAAI,cAAc,QAAQ;YACvB,iDAAiD;YACjD,MAAM,YAAY,IAAA,qJAAc,EAAC,MAAM;YAEvC,iCAAiC;YACjC,eAAe,MAAM,IAAI,CAAC,IAAI,IAAI;YAElC,IAAI,aAAa,MAAM,GAAG,aAAa;gBACpC,OAAO,gJAAY,CAAC,IAAI,CACtB;oBAAE,OAAO,CAAC,yBAAyB,EAAE,aAAa,MAAM,CAAC,MAAM,EAAE,YAAY,UAAU,CAAC;gBAAC,GACzF;oBAAE,QAAQ;gBAAI;YAEnB;YAEA,gCAAgC;YAChC,iCAAiC;YACjC,+DAA+D;YAE/D,UAAU,aAAa,GAAG,CAAC,CAAA;gBACxB,MAAM,MAAM,IAAI,MAAM,UAAU,MAAM,EAAE,IAAI,CAAC;gBAE7C,UAAU,OAAO,CAAC,CAAC,UAAU;oBAC1B,kFAAkF;oBAClF,yEAAyE;oBACzE,uEAAuE;oBACvE,IAAI,SAAS,WAAW,GAAG,QAAQ,CAAC,KAAK,WAAW,KAAK;wBACtD,GAAG,CAAC,KAAK,GAAG,GAAG,2BAA2B;oBAC7C;gBACH;gBAEA,mBAAmB;gBACnB,IAAI,MAAM;gBACV,KAAI,MAAM,KAAK,IAAK,OAAO,IAAE;gBAC7B,MAAM,KAAK,IAAI,CAAC;gBAChB,IAAI,MAAM,GAAG;oBACV,IAAI,IAAI,IAAE,GAAG,IAAE,IAAI,MAAM,EAAE,IAAK,GAAG,CAAC,EAAE,IAAI;gBAC7C;gBAEA,OAAO;YACV;YAEA,2EAA2E;YAC3E,aAAa;QAEhB,OAAO;YACJ,iEAAiE;YACjE,eAAe;YACf,MAAM,aAAa,IAAA,8IAAS,EAAC;YAC7B,UAAU,WAAW,OAAO;YAC5B,aAAa,WAAW,UAAU;QACrC;QAEA,6BAA6B;QAC7B,IAAI;QAEJ,IAAI,cAAc,UAAU;YAC1B,MAAM,SAAS,IAAA,8IAAa,EAAC,SAAS,aAAa;gBAAE,UAAU;YAAG;YAClE,qBAAqB,OAAO,QAAQ;QACtC,OAAO;YACL,qBAAqB,IAAA,0JAAmB,EAAC,SAAS;QACpD;QAEA,gCAAgC;QAChC,MAAM,QAAuB,aAAa,GAAG,CAAC,CAAC,OAAO,MAAQ,CAAC;gBAC7D,IAAI,CAAC,KAAK,EAAE,KAAK;gBACjB,MAAM;gBACN,SAAS,kBAAkB,CAAC,IAAI;YAClC,CAAC;QAED,2CAA2C;QAC3C,sDAAsD;QACtD,MAAM,QAAuB,EAAE;QAC/B,MAAM,sBAAsB,KAAK,mCAAmC;QAEpE,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,MAAM,EAAE,IAAK;YACvC,IAAK,IAAI,IAAI,IAAI,GAAG,IAAI,QAAQ,MAAM,EAAE,IAAK;gBAC3C,MAAM,aAAa,IAAA,qJAAgB,EAAC,OAAO,CAAC,EAAE,EAAE,OAAO,CAAC,EAAE;gBAE1D,IAAI,aAAa,qBAAqB;oBACpC,MAAM,IAAI,CAAC;wBACT,QAAQ,CAAC,KAAK,EAAE,GAAG;wBACnB,QAAQ,CAAC,KAAK,EAAE,GAAG;wBACnB,OAAO;oBACT;gBACF;YACF;QACF;QAEA,uCAAuC;QACvC,MAAM,aAAa,IAAI;QACvB,mBAAmB,OAAO,CAAC,CAAC,SAAS;YACnC,IAAI,CAAC,WAAW,GAAG,CAAC,UAAU;gBAC5B,WAAW,GAAG,CAAC,SAAS,EAAE;YAC5B;YACA,WAAW,GAAG,CAAC,SAAU,IAAI,CAAC;QAChC;QAEA,MAAM,WAA0B,MAAM,IAAI,CAAC,WAAW,OAAO,IAAI,GAAG,CAAC,CAAC,CAAC,IAAI,QAAQ;YACjF,+EAA+E;YAC/E,IAAI;YAEJ,IAAI,cAAc,QAAQ;gBACtB,yCAAyC;gBACzC,sGAAsG;gBACtG,yGAAyG;gBAEzG,+EAA+E;gBAC/E,mDAAmD;gBACnD,yCAAyC;gBAEzC,0FAA0F;gBAC1F,yFAAyF;gBACzF,yGAAyG;gBACzG,gDAAgD;gBAEhD,wEAAwE;gBACxE,qFAAqF;gBACrF,MAAM,YAAY,IAAA,sJAAe,EAAC,IAAA,+IAAQ,EAAC,MAAM;gBAEjD,MAAM,UAAU,IAAI;gBACpB,UAAU,OAAO,CAAC,CAAA;oBACf,QAAQ,GAAG,CAAC,GAAG,CAAC,QAAQ,GAAG,CAAC,MAAM,CAAC,IAAI;gBAC1C;gBAEA,eAAe,QAAQ,GAAG,CAAC,CAAA,MAAO,CAAC;wBAC/B,MAAM,YAAY,CAAC,IAAI;wBACvB,QAAQ,QAAQ,GAAG,CAAC,YAAY,CAAC,IAAI,KAAK;oBAC9C,CAAC,GACA,IAAI,CAAC,CAAC,GAAG,IAAM,EAAE,MAAM,GAAG,EAAE,MAAM,EAAE,oBAAoB;iBACxD,KAAK,CAAC,GAAG;YAEd,OAAO;gBACH,6CAA6C;gBAC7C,eAAe,sBAAsB,SAAS,cAAc,SAAS,YAAY;YACrF;YAEA,OAAO;gBACL;gBACA,MAAM,QAAQ,MAAM;gBACpB,OAAO;gBACP,OAAO,IAAA,oIAAe,EAAC;YACzB;QACF;QAEA,MAAM,iBAAiB,KAAK,GAAG,KAAK;QAEpC,MAAM,SAAwB;YAC5B;YACA;YACA;YACA,aAAa,aAAa,MAAM;YAChC;YACA,WAAW,cAAc,WAAW,YAAY;QAClD;QAEA,OAAO,gJAAY,CAAC,IAAI,CAAC;IAE3B,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,qBAAqB;QACnC,OAAO,gJAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAA0C,GACnD;YAAE,QAAQ;QAAI;IAElB;AACF;AAEA,SAAS,sBACP,OAAiB,EACjB,MAAgB,EAChB,OAAmB,EACnB,UAAoB,EACpB,IAAY;IAEZ,+DAA+D;IAC/D,MAAM,aAAa,IAAI;IAEvB,KAAK,MAAM,OAAO,QAAS;QACzB,MAAM,QAAQ,MAAM,CAAC,IAAI;QACzB,MAAM,SAAS,OAAO,CAAC,IAAI;QAE3B,mDAAmD;QACnD,MAAM,QAAQ,MAAM,KAAK,CAAC;QAC1B,MAAM,OAAO,CAAC,CAAA;YACZ,MAAM,WAAW,WAAW,OAAO,CAAC;YACpC,IAAI,aAAa,CAAC,GAAG;gBACnB,MAAM,aAAa,MAAM,CAAC,SAAS;gBACnC,MAAM,eAAe,WAAW,GAAG,CAAC,SAAS;gBAC7C,WAAW,GAAG,CAAC,MAAM,eAAe;YACtC;QACF;IACF;IAEA,2BAA2B;IAC3B,MAAM,cAAc,QAAQ,MAAM;IAClC,WAAW,OAAO,CAAC,CAAC,OAAO;QACzB,WAAW,GAAG,CAAC,MAAM,QAAQ;IAC/B;IAEA,kCAAkC;IAClC,OAAO,MAAM,IAAI,CAAC,WAAW,OAAO,IACjC,IAAI,CAAC,CAAC,GAAG,IAAM,CAAC,CAAC,EAAE,GAAG,CAAC,CAAC,EAAE,EAC1B,KAAK,CAAC,GAAG,MACT,GAAG,CAAC,CAAC,CAAC,MAAM,OAAO,GAAK,CAAC;YACxB;YACA,QAAQ,KAAK,KAAK,CAAC,SAAS,OAAO;QACrC,CAAC;AACL"}}]
}